#app.py

# Aplicaci√≥n para detecci√≥n y an√°lisis de riesgo en veh√≠culos seg√∫n la cantidad de ocupantes.

# Materia: Procesamiento de Im√°genes - Parcial 1
# Alumno: Carlos Ezequiel Leiva
# Docente: Lucas De Rito


#Se utiliza YOLOv8 para detectar autos, motos, bicicletas y personas. Se eval√∫a el nivel de riesgo (bajo, medio o alto) en funci√≥n del tipo de veh√≠culo y la cantidad de personas detectadas.

import streamlit as st
import cv2
import os
import numpy as np
from PIL import Image
from logica import evaluar_riesgo
from ultralytics import YOLO

# CARGA DE MODELOS
# Carga del modelo de detecci√≥n de objetos (veh√≠culos)
# El modelo 'modelo_objetos' detecta veh√≠culos como autos, motos, bicicletas
modelo_objetos = YOLO("yolov8n.pt")

# Carga del modelo de estimaci√≥n de pose para detectar personas a trav√©s de keypoints
# El modelo 'modelo_pose' identifica la postura de las personas
modelo_pose = YOLO("yolov8n-pose.pt")

# CONFIGURACI√ìN DE STREAMLIT

# Configuraci√≥n de la p√°gina de Streamlit en modo 'wide'
st.set_page_config(layout="wide")

# T√≠tulo principal de la app
st.title("üöó Detecci√≥n de Riesgo Vehicular - Seg√∫n N¬∞ de Ocupantes")

# Lista de im√°genes precargadas ubicadas en la carpeta IMG/
# Se cargan las im√°genes con nombres '1.jpg', '2.jpg', ..., '8.jpg'
imagenes_precargadas = [f"{i}.jpg" for i in range(1, 9)]

# Divisi√≥n de la pantalla en dos columnas (para mostrar el selector de imagen y el resultado de riesgo)
top_col1, top_col2 = st.columns([1, 2])

# SELECCI√ìN DE IMAGEN
with top_col1:
    # Se crea un selector para que el usuario elija una imagen de las precargadas
    imagen_seleccionada = st.selectbox("Selecciona una imagen", imagenes_precargadas)

# PROCESAMIENTO DE IMAGEN
if imagen_seleccionada:
    # Obtener la ruta de la imagen seleccionada
    ruta_imagen = os.path.join("IMG", imagen_seleccionada)

    if not os.path.exists(ruta_imagen):
        # Si la imagen no se encuentra en la ruta, mostrar un error
        st.error("No se encuentra la imagen. Verifica la ruta.")
    else:
        # Leer la imagen con OpenCV y convertirla a formato RGB
        imagen = cv2.imread(ruta_imagen)
        imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)

        # DETECCI√ìN DE VEH√çCULOS 

        # Modelo de detecci√≥n de objetos para predecir veh√≠culos en la imagen
        resultado_objetos = modelo_objetos.predict(source=imagen_rgb, conf=0.3)[0]
        clases = resultado_objetos.names  # Clases de objetos detectados (autos, motos, etc.)
        clases_detectadas = [clases[int(c)] for c in resultado_objetos.boxes.cls]

        # Contar la cantidad de autos, motos y bicicletas detectados
        autos = clases_detectadas.count("car")
        motos = clases_detectadas.count("motorcycle")
        bicicletas = clases_detectadas.count("bicycle")

        # Mostrar el resumen de veh√≠culos detectados
        st.write(f"Veh√≠culos detectados: üöó {autos} autos | üèçÔ∏è {motos} motos | üö≤ {bicicletas} bicicletas")

        # DETECCI√ìN DE PERSONAS (POSE ESTIMATION)
        # Modelo de pose para detectar personas y sus keypoints
        resultado_pose = modelo_pose.predict(source=imagen_rgb, conf=0.3)[0]
        personas_detectadas = 0

        # Se considera una persona v√°lida si tiene al menos 5 keypoints visibles
        if resultado_pose.keypoints is not None and resultado_pose.keypoints.shape[0] > 0:
            for kpts in resultado_pose.keypoints.data:
                visibles = kpts[:, 2] > 0.5  # Verifica la visibilidad de los keypoints
                if visibles.sum() >= 5:
                    personas_detectadas += 1

        # DIBUJO DE RESULTADOS EN LA IMAGEN 

        # Combinamos los resultados de pose en la imagen original
        imagen_combinada = resultado_pose.plot().copy()

        # Dibujo de los cuadros de los veh√≠culos detectados en la imagen combinada
        for box, cls in zip(resultado_objetos.boxes.xyxy, resultado_objetos.boxes.cls):
            x1, y1, x2, y2 = map(int, box)
            clase = clases[int(cls)]

            if clase in ["car", "motorcycle", "bicycle"]:
                # Colores personalizados para cada tipo de veh√≠culo
                color = (
                    (0, 255, 255) if clase == "car" else           # Celeste para autos
                    (255, 255, 0) if clase == "motorcycle" else    # Amarillo para motos
                    (138, 43, 226)                                 # Violeta para bicis
                )
                etiqueta = "Auto" if clase == "car" else "Moto" if clase == "motorcycle" else "Bici"
                cv2.rectangle(imagen_combinada, (x1, y1), (x2, y2), color, 2)
                cv2.putText(imagen_combinada, etiqueta, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

        #EVALUACI√ìN DEL RIESGO
        # Evaluaci√≥n del nivel de riesgo basado en el tipo de veh√≠culos y la cantidad de personas detectadas
        mensaje_riesgo = evaluar_riesgo(autos, motos, bicicletas, personas_detectadas)

        # Color del mensaje de riesgo (rojo para alto, naranja para medio, verde para bajo)
        if "ALTO" in mensaje_riesgo.upper():
            color_riesgo = "red"
        elif "MEDIO" in mensaje_riesgo.upper():
            color_riesgo = "orange"
        else:
            color_riesgo = "green"

        #VISUALIZACI√ìN DE RESULTADOS
        # mostrar la imagen original y la imagen procesada (con resultados de detecci√≥n)
        col1, col2 = st.columns(2)
        with col1:
            st.image(imagen_rgb, caption="üñºÔ∏è Imagen Original", width=500)
        with col2:
            st.image(imagen_combinada, caption="‚úÖ Imagen Procesada (Personas + Veh√≠culos)", width=500)

        # Mostrar el mensaje de riesgo al lado del selector de im√°genes
        with top_col2:
            st.markdown(
                f"<h2 style='color:{color_riesgo}; text-align:center'>{mensaje_riesgo}</h2>",
                unsafe_allow_html=True
            )